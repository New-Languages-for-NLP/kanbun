{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87ea128c",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/New-Languages-for-NLP/kanbun/blob/main/New_Language_Training_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061ba15a",
      "metadata": {
        "id": "061ba15a"
      },
      "source": [
        "For full documentation on this project, see [here](https://new-languages-for-nlp.github.io/course-materials/w2/projects.html)\n",
        " \n",
        "This notebook: \n",
        "- Loads project file from GitHub\n",
        "- Loads assets from GitHub repo\n",
        "- installs the custom language object \n",
        "- converts the training data to spaCy binary\n",
        "- configure the project.yml file \n",
        "- train the model \n",
        "- assess performance \n",
        "- package the model (or push to huggingface) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "psP7JdcRLR7n",
      "metadata": {
        "id": "psP7JdcRLR7n"
      },
      "source": [
        "# 1 Prepare the Notebook Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ba9e5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ba9e5a",
        "outputId": "f77a18fc-bf68-45af-92dd-58bbc8c46939"
      },
      "outputs": [],
      "source": [
        "#@title Colab comes with spaCy v2, needs upgrade to v3\n",
        "GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Install spaCy v3 and libraries for GPUs and transformers\n",
        "!pip install spacy --upgrade\n",
        "if GPU:\n",
        "    !pip install 'spacy[transformers,cuda111]'\n",
        "!pip install wandb spacy-huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WfsEKZv6ErlG",
      "metadata": {
        "id": "WfsEKZv6ErlG"
      },
      "source": [
        "The notebook will pull project files from your GitHub repository.  \n",
        "\n",
        "Note that you need to set the langugage (lang), treebank (same as the repo name), test_size and package name in the project.yml file in your repository.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0bda8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c0bda8e",
        "outputId": "5868c050-e551-4d46-c776-56fb12c7e61a"
      },
      "outputs": [],
      "source": [
        "private_repo = False #@param {type:\"boolean\"}\n",
        "repo_name = \"kanbun\" #@param {type:\"string\"}\n",
        "\n",
        "!rm -rf /content/newlang_project\n",
        "!rm -rf $repo_name\n",
        "if private_repo:\n",
        "    git_access_token = \"\" #@param {type:\"string\"}\n",
        "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
        "    !git clone $git_url  -b main\n",
        "    !cp -r ./$repo_name/newlang_project .  \n",
        "    !mkdir newlang_project/assets/\n",
        "    !mkdir newlang_project/configs/\n",
        "    !mkdir newlang_project/corpus/\n",
        "    !mkdir newlang_project/metrics/\n",
        "    !mkdir newlang_project/packages/\n",
        "    !mkdir newlang_project/training/\n",
        "    !mkdir newlang_project/assets/$repo_name\n",
        "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
        "    !rm -rf ./$repo_name\n",
        "else:\n",
        "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch main\n",
        "    !python -m spacy project assets /content/newlang_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc13741",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dc13741",
        "outputId": "fd2b51b9-479f-41fe-b86a-e4c7d32860c1"
      },
      "outputs": [],
      "source": [
        "# Install the custom language object from Cadet \n",
        "!python -m spacy project run install /content/newlang_project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qU_AqRK6LZrF",
      "metadata": {
        "id": "qU_AqRK6LZrF"
      },
      "source": [
        "# 2 Prepare the Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HPjD0SN6iAvc",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjD0SN6iAvc",
        "outputId": "f38fc69a-ba2f-4f35-a2c3-3cc440a2b68f"
      },
      "outputs": [],
      "source": [
        "#@title (optional) cell to corrects a problem when your tokens have no pos value\n",
        "%%writefile /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n",
        "import re\n",
        "\n",
        "from .conll_ner_to_docs import n_sents_info\n",
        "from ...training import iob_to_biluo, biluo_tags_to_spans\n",
        "from ...tokens import Doc, Token, Span\n",
        "from ...vocab import Vocab\n",
        "from wasabi import Printer\n",
        "\n",
        "\n",
        "def conllu_to_docs(\n",
        "    input_data,\n",
        "    n_sents=10,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "    merge_subtokens=False,\n",
        "    no_print=False,\n",
        "    **_\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert conllu files into JSON format for use with train cli.\n",
        "    append_morphology parameter enables appending morphology to tags, which is\n",
        "    useful for languages such as Spanish, where UD tags are not so rich.\n",
        "\n",
        "    Extract NER tags if available and convert them so that they follow\n",
        "    BILUO and the Wikipedia scheme\n",
        "    \"\"\"\n",
        "    MISC_NER_PATTERN = \"^((?:name|NE)=)?([BILU])-([A-Z_]+)|O$\"\n",
        "    msg = Printer(no_print=no_print)\n",
        "    n_sents_info(msg, n_sents)\n",
        "    sent_docs = read_conllx(\n",
        "        input_data,\n",
        "        append_morphology=append_morphology,\n",
        "        ner_tag_pattern=MISC_NER_PATTERN,\n",
        "        ner_map=ner_map,\n",
        "        merge_subtokens=merge_subtokens,\n",
        "    )\n",
        "    sent_docs_to_merge = []\n",
        "    for sent_doc in sent_docs:\n",
        "        sent_docs_to_merge.append(sent_doc)\n",
        "        if len(sent_docs_to_merge) % n_sents == 0:\n",
        "            yield Doc.from_docs(sent_docs_to_merge)\n",
        "            sent_docs_to_merge = []\n",
        "    if sent_docs_to_merge:\n",
        "        yield Doc.from_docs(sent_docs_to_merge)\n",
        "\n",
        "\n",
        "def has_ner(input_data, ner_tag_pattern):\n",
        "    \"\"\"\n",
        "    Check the MISC column for NER tags.\n",
        "    \"\"\"\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            for line in lines:\n",
        "                parts = line.split(\"\\t\")\n",
        "                id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "                for misc_part in misc.split(\"|\"):\n",
        "                    if re.match(ner_tag_pattern, misc_part):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def read_conllx(\n",
        "    input_data,\n",
        "    append_morphology=False,\n",
        "    merge_subtokens=False,\n",
        "    ner_tag_pattern=\"\",\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Yield docs, one for each sentence\"\"\"\n",
        "    vocab = Vocab()  # need vocab to make a minimal Doc\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            doc = conllu_sentence_to_doc(\n",
        "                vocab,\n",
        "                lines,\n",
        "                ner_tag_pattern,\n",
        "                merge_subtokens=merge_subtokens,\n",
        "                append_morphology=append_morphology,\n",
        "                ner_map=ner_map,\n",
        "            )\n",
        "            yield doc\n",
        "\n",
        "\n",
        "def get_entities(lines, tag_pattern, ner_map=None):\n",
        "    \"\"\"Find entities in the MISC column according to the pattern and map to\n",
        "    final entity type with `ner_map` if mapping present. Entity tag is 'O' if\n",
        "    the pattern is not matched.\n",
        "\n",
        "    lines (str): CONLL-U lines for one sentences\n",
        "    tag_pattern (str): Regex pattern for entity tag\n",
        "    ner_map (dict): Map old NER tag names to new ones, '' maps to O.\n",
        "    RETURNS (list): List of BILUO entity tags\n",
        "    \"\"\"\n",
        "    miscs = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_ or \".\" in id_:\n",
        "            continue\n",
        "        miscs.append(misc)\n",
        "\n",
        "    iob = []\n",
        "    for misc in miscs:\n",
        "        iob_tag = \"O\"\n",
        "        for misc_part in misc.split(\"|\"):\n",
        "            tag_match = re.match(tag_pattern, misc_part)\n",
        "            if tag_match:\n",
        "                prefix = tag_match.group(2)\n",
        "                suffix = tag_match.group(3)\n",
        "                if prefix and suffix:\n",
        "                    iob_tag = prefix + \"-\" + suffix\n",
        "                    if ner_map:\n",
        "                        suffix = ner_map.get(suffix, suffix)\n",
        "                        if suffix == \"\":\n",
        "                            iob_tag = \"O\"\n",
        "                        else:\n",
        "                            iob_tag = prefix + \"-\" + suffix\n",
        "                break\n",
        "        iob.append(iob_tag)\n",
        "    return iob_to_biluo(iob)\n",
        "\n",
        "\n",
        "def conllu_sentence_to_doc(\n",
        "    vocab,\n",
        "    lines,\n",
        "    ner_tag_pattern,\n",
        "    merge_subtokens=False,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Create an Example from the lines for one CoNLL-U sentence, merging\n",
        "    subtokens and appending morphology to tags if required.\n",
        "\n",
        "    lines (str): The non-comment lines for a CoNLL-U sentence\n",
        "    ner_tag_pattern (str): The regex pattern for matching NER in MISC col\n",
        "    RETURNS (Example): An example containing the annotation\n",
        "    \"\"\"\n",
        "    # create a Doc with each subtoken as its own token\n",
        "    # if merging subtokens, each subtoken orth is the merged subtoken form\n",
        "    if not Token.has_extension(\"merged_orth\"):\n",
        "        Token.set_extension(\"merged_orth\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_lemma\"):\n",
        "        Token.set_extension(\"merged_lemma\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_morph\"):\n",
        "        Token.set_extension(\"merged_morph\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_spaceafter\"):\n",
        "        Token.set_extension(\"merged_spaceafter\", default=\"\")\n",
        "    words, spaces, tags, poses, morphs, lemmas = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    subtok_word = \"\"\n",
        "    in_subtok = False\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \".\" in id_:\n",
        "            continue\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "            subtok_word = word\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_spaceafter = \"SpaceAfter=No\" not in misc\n",
        "            continue\n",
        "        if merge_subtokens and in_subtok:\n",
        "            words.append(subtok_word)\n",
        "        else:\n",
        "            words.append(word)\n",
        "        if in_subtok:\n",
        "            if id_ == subtok_end:\n",
        "                spaces.append(subtok_spaceafter)\n",
        "            else:\n",
        "                spaces.append(False)\n",
        "        elif \"SpaceAfter=No\" in misc:\n",
        "            spaces.append(False)\n",
        "        else:\n",
        "            spaces.append(True)\n",
        "        if in_subtok and id_ == subtok_end:\n",
        "            subtok_word = \"\"\n",
        "            in_subtok = False\n",
        "        id_ = int(id_) - 1\n",
        "        head = (int(head) - 1) if head not in (\"0\", \"_\") else id_\n",
        "        tag = pos if tag == \"_\" else tag\n",
        "        morph = morph if morph != \"_\" else \"\"\n",
        "        dep = \"ROOT\" if dep == \"root\" else dep\n",
        "        lemmas.append(lemma)\n",
        "        if pos == \"_\":\n",
        "            pos = \"\"\n",
        "        poses.append(pos)\n",
        "        tags.append(tag)\n",
        "        morphs.append(morph)\n",
        "        heads.append(head)\n",
        "        deps.append(dep)\n",
        "\n",
        "    doc = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        lemmas=lemmas,\n",
        "        morphs=morphs,\n",
        "        heads=heads,\n",
        "    )\n",
        "    for i in range(len(doc)):\n",
        "        doc[i]._.merged_orth = words[i]\n",
        "        doc[i]._.merged_morph = morphs[i]\n",
        "        doc[i]._.merged_lemma = lemmas[i]\n",
        "        doc[i]._.merged_spaceafter = spaces[i]\n",
        "    ents = get_entities(lines, ner_tag_pattern, ner_map)\n",
        "    doc.ents = biluo_tags_to_spans(doc, ents)\n",
        "\n",
        "    if merge_subtokens:\n",
        "        doc = merge_conllu_subtokens(lines, doc)\n",
        "\n",
        "    # create final Doc from custom Doc annotation\n",
        "    words, spaces, tags, morphs, lemmas, poses = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    for i, t in enumerate(doc):\n",
        "        words.append(t._.merged_orth)\n",
        "        lemmas.append(t._.merged_lemma)\n",
        "        spaces.append(t._.merged_spaceafter)\n",
        "        morphs.append(t._.merged_morph)\n",
        "        if append_morphology and t._.merged_morph:\n",
        "            tags.append(t.tag_ + \"__\" + t._.merged_morph)\n",
        "        else:\n",
        "            tags.append(t.tag_)\n",
        "        poses.append(t.pos_)\n",
        "        heads.append(t.head.i)\n",
        "        deps.append(t.dep_)\n",
        "\n",
        "    doc_x = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        morphs=morphs,\n",
        "        lemmas=lemmas,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        heads=heads,\n",
        "    )\n",
        "    doc_x.ents = [Span(doc_x, ent.start, ent.end, label=ent.label) for ent in doc.ents]\n",
        "\n",
        "    return doc_x\n",
        "\n",
        "\n",
        "def merge_conllu_subtokens(lines, doc):\n",
        "    # identify and process all subtoken spans to prepare attrs for merging\n",
        "    subtok_spans = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_:\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_span = doc[int(subtok_start) - 1 : int(subtok_end)]\n",
        "            subtok_spans.append(subtok_span)\n",
        "            # create merged tag, morph, and lemma values\n",
        "            tags = []\n",
        "            morphs = {}\n",
        "            lemmas = []\n",
        "            for token in subtok_span:\n",
        "                tags.append(token.tag_)\n",
        "                lemmas.append(token.lemma_)\n",
        "                if token._.merged_morph:\n",
        "                    for feature in token._.merged_morph.split(\"|\"):\n",
        "                        field, values = feature.split(\"=\", 1)\n",
        "                        if field not in morphs:\n",
        "                            morphs[field] = set()\n",
        "                        for value in values.split(\",\"):\n",
        "                            morphs[field].add(value)\n",
        "            # create merged features for each morph field\n",
        "            for field, values in morphs.items():\n",
        "                morphs[field] = field + \"=\" + \",\".join(sorted(values))\n",
        "            # set the same attrs on all subtok tokens so that whatever head the\n",
        "            # retokenizer chooses, the final attrs are available on that token\n",
        "            for token in subtok_span:\n",
        "                token._.merged_orth = token.orth_\n",
        "                token._.merged_lemma = \" \".join(lemmas)\n",
        "                token.tag_ = \"_\".join(tags)\n",
        "                token._.merged_morph = \"|\".join(sorted(morphs.values()))\n",
        "                token._.merged_spaceafter = (\n",
        "                    True if subtok_span[-1].whitespace_ else False\n",
        "                )\n",
        "\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in subtok_spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563fdc94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "563fdc94",
        "outputId": "eefb7496-da69-4e02-d5dc-978b8783bf3e"
      },
      "outputs": [],
      "source": [
        "# Convert the conllu files from inception to spaCy binary format\n",
        "# Read the conll files with ner data and as ents to spaCy docs \n",
        "!python -m spacy project run convert /content/newlang_project -F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9519c858",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9519c858",
        "outputId": "37b356d3-bce9-4440-f3eb-48e088c4153f"
      },
      "outputs": [],
      "source": [
        "# test/train split \n",
        "!python -m spacy project run split /content/newlang_project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4feefe6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4feefe6f",
        "outputId": "8df2bdf8-e7a8-4023-9da0-61fd2221a495"
      },
      "outputs": [],
      "source": [
        "# Debug the data\n",
        "!python -m spacy project run debug /content/newlang_project "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151-cj1dLgAD",
      "metadata": {
        "id": "151-cj1dLgAD"
      },
      "source": [
        "# 3 Model Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b490a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38b490a4",
        "outputId": "eb7c0673-3dd5-4ddc-ce1e-e093c346db5a"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "!python -m spacy project run train /content/newlang_project "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ynXr8vlXqCxv",
      "metadata": {
        "id": "ynXr8vlXqCxv"
      },
      "source": [
        "If you get `ValueError: Could not find gold transition - see logs above.`  \n",
        "You may not have sufficent data to train on: https://github.com/explosion/spaCy/discussions/7282"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018362d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "018362d8",
        "outputId": "1198836a-6d4a-4f3e-8907-b7757f71e24a"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using the test data\n",
        "!python -m spacy project run evaluate /content/newlang_project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gg1sLlVrgiyu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg1sLlVrgiyu",
        "outputId": "09aa97b7-f7b3-404b-f965-b9140b60e3e6"
      },
      "outputs": [],
      "source": [
        "# Find the path for your meta.json file\n",
        "# You'll need to add newlang_project/ +  the path from the training step just after \"✔ Saved pipeline to output directory\"\n",
        "!ls newlang_project/training/urban-giggle/model-last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3zGCTURr9JE6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zGCTURr9JE6",
        "outputId": "861443f2-a93b-4022-dd1b-10faa79599be"
      },
      "outputs": [],
      "source": [
        "#Update meta.json\n",
        "import spacy \n",
        "import srsly \n",
        "\n",
        "# Change path to match that from the training cell where it says \"✔ Saved pipeline to output directory\"\n",
        "meta_path = \"newlang_project/training/urban-giggle/model-last/meta.json\"\n",
        "\n",
        "# Replace values below for your project\n",
        "my_meta = { \n",
        "    \"lang\":\"yi\",\n",
        "    \"name\":\"yiddish_sm\",\n",
        "    \"version\":\"0.0.1\",\n",
        "    \"description\":\"Yiddish pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, lemmatizer.\",\n",
        "    \"author\":\"New Languages for NLP\",\n",
        "    \"email\":\"newnlp@princeton.edu\",\n",
        "    \"url\":\"https://newnlp.princeton.edu\",\n",
        "    \"license\":\"MIT\", \n",
        "    }\n",
        "meta = spacy.util.load_meta(meta_path)\n",
        "meta.update(my_meta)\n",
        "srsly.write_json(meta_path, meta)\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JM309FhNVAeb",
      "metadata": {
        "id": "JM309FhNVAeb"
      },
      "source": [
        "### Download the trained model to your computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1d6f36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e1d6f36",
        "outputId": "5fd6b19c-0f93-47a9-eae3-ee1b67abfa3e"
      },
      "outputs": [],
      "source": [
        "# Save the model to disk in a format that can be easily  downloaded and re-used.\n",
        "!python -m spacy package ./newlang_project/training/urban-giggle/model-last newlang_project/export "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d32abf2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8d32abf2",
        "outputId": "24a11e7a-800a-4fe8-ed12-c26f8fb15146"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# replace with the path in the previous cell under \"✔ Successfully created zipped Python package\"\n",
        "files.download('newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz')\n",
        "\n",
        "# once on your computer, you can pip install en_pipeline-0.0.0.tar.gz\n",
        "# Add to 4_trained_models folder in GitHub"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "New Language Training (Colab).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
