{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/New-Languages-for-NLP/repo-template/blob/main/New_Language_Training_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061ba15a",
      "metadata": {
        "id": "061ba15a"
      },
      "source": [
        "For full documentation on this project, see [here](https://new-languages-for-nlp.github.io/course-materials/w2/projects.html)\n",
        " \n",
        "This notebook: \n",
        "- Loads project file from GitHub\n",
        "- Loads assets from GitHub repo\n",
        "- installs the custom language object \n",
        "- converts the training data to spaCy binary\n",
        "- configure the project.yml file \n",
        "- train the model \n",
        "- assess performance \n",
        "- package the model (or push to huggingface) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Prepare the Notebook Environment"
      ],
      "metadata": {
        "id": "psP7JdcRLR7n"
      },
      "id": "psP7JdcRLR7n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a0ba9e5a",
      "metadata": {
        "id": "a0ba9e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77a18fc-bf68-45af-92dd-58bbc8c46939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 18.3 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 44.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.6 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0\n",
            "Requirement already satisfied: spacy[cuda111,transformers] in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.19.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (4.62.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.6.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.8.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.10.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (8.0.13)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (1.8.2)\n",
            "Requirement already satisfied: cupy-cuda111<10.0.0,>=5.0.0b4 in /usr/local/lib/python3.7/dist-packages (from spacy[cuda111,transformers]) (9.4.0)\n",
            "Collecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.3-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[cuda111,transformers]) (3.6.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda111<10.0.0,>=5.0.0b4->spacy[cuda111,transformers]) (0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[cuda111,transformers]) (3.0.6)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[cuda111,transformers]) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda111,transformers]) (3.0.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (1.10.0+cu111)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 9.2 MB/s \n",
            "\u001b[?25hCollecting transformers<4.13.0,>=3.4.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.13.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 479 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.13.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.13.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 30.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy[cuda111,transformers]) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[cuda111,transformers]) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.13.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.13.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->spacy[cuda111,transformers]) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 spacy-alignments-0.8.4 spacy-transformers-1.1.3 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting spacy-huggingface-hub\n",
            "  Downloading spacy_huggingface_hub-0.0.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy-huggingface-hub) (0.8.2)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->spacy-huggingface-hub) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->spacy-huggingface-hub) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->spacy-huggingface-hub) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->spacy-huggingface-hub) (3.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub==0.0.12->spacy-huggingface-hub) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub==0.0.12->spacy-huggingface-hub) (3.6.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=2f1fe110f45add9b2c65ae0ed02fb8df39ea2e22adc79c83ee15d219a2ede6f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=494bc2b47542ab67d30f2d6893bc166e39ead63e1e8bb9106ecf5c38f179c4a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, typer, subprocess32, shortuuid, sentry-sdk, pathtools, huggingface-hub, GitPython, docker-pycreds, configparser, wandb, spacy-huggingface-hub\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.4.0\n",
            "    Uninstalling typer-0.4.0:\n",
            "      Successfully uninstalled typer-0.4.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.2.1\n",
            "    Uninstalling huggingface-hub-0.2.1:\n",
            "      Successfully uninstalled huggingface-hub-0.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.12.5 requires huggingface-hub<1.0,>=0.1.0, but you have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 huggingface-hub-0.0.12 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 spacy-huggingface-hub-0.0.6 subprocess32-3.5.4 typer-0.3.2 wandb-0.12.9 yaspin-2.1.0\n"
          ]
        }
      ],
      "source": [
        "#@title Colab comes with spaCy v2, needs upgrade to v3\n",
        "GPU = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Install spaCy v3 and libraries for GPUs and transformers\n",
        "!pip install spacy --upgrade\n",
        "if GPU:\n",
        "    !pip install 'spacy[transformers,cuda111]'\n",
        "!pip install wandb spacy-huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook will pull project files from your GitHub repository.  \n",
        "\n",
        "Note that you need to set the langugage (lang), treebank (same as the repo name), test_size and package name in the project.yml file in your repository.  "
      ],
      "metadata": {
        "id": "WfsEKZv6ErlG"
      },
      "id": "WfsEKZv6ErlG"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7c0bda8e",
      "metadata": {
        "id": "7c0bda8e",
        "outputId": "5868c050-e551-4d46-c776-56fb12c7e61a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Cloned 'newlang_project' from New-Languages-for-NLP/repo-template\u001b[0m\n",
            "/content/newlang_project\n",
            "\u001b[38;5;2m✔ Your project is now ready!\u001b[0m\n",
            "To fetch the assets, run:\n",
            "python -m spacy project assets /content/newlang_project\n",
            "\u001b[38;5;4mℹ Fetching 1 asset(s)\u001b[0m\n",
            "\u001b[38;5;2m✔ Downloaded asset /content/newlang_project/assets/urban-giggle\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "private_repo = False #@param {type:\"boolean\"}\n",
        "repo_name = \"repo-template\" #@param {type:\"string\"}\n",
        "\n",
        "!rm -rf /content/newlang_project\n",
        "!rm -rf $repo_name\n",
        "if private_repo:\n",
        "    git_access_token = \"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\" #@param {type:\"string\"}\n",
        "    git_url = f\"https://{git_access_token}@github.com/New-Languages-for-NLP/{repo_name}/\"\n",
        "    !git clone $git_url  -b main\n",
        "    !cp -r ./$repo_name/newlang_project .  \n",
        "    !mkdir newlang_project/assets/\n",
        "    !mkdir newlang_project/configs/\n",
        "    !mkdir newlang_project/corpus/\n",
        "    !mkdir newlang_project/metrics/\n",
        "    !mkdir newlang_project/packages/\n",
        "    !mkdir newlang_project/training/\n",
        "    !mkdir newlang_project/assets/$repo_name\n",
        "    !cp -r ./$repo_name/* newlang_project/assets/$repo_name/\n",
        "    !rm -rf ./$repo_name\n",
        "else:\n",
        "    !python -m spacy project clone newlang_project --repo https://github.com/New-Languages-for-NLP/$repo_name --branch main\n",
        "    !python -m spacy project assets /content/newlang_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4dc13741",
      "metadata": {
        "id": "4dc13741",
        "outputId": "fd2b51b9-479f-41fe-b86a-e4c7d32860c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== install ==================================\u001b[0m\n",
            "Running command: rm -rf lang\n",
            "Running command: mkdir lang\n",
            "Running command: mkdir lang/yi\n",
            "Running command: cp -r assets/urban-giggle/2_new_language_object/ lang/yi/yi\n",
            "Running command: mv lang/yi/yi/setup.py lang/yi/\n",
            "Running command: /usr/bin/python3 -m pip install -e lang/yi\n",
            "Obtaining file:///content/newlang_project/lang/yi\n",
            "Installing collected packages: yi\n",
            "  Running setup.py develop for yi\n",
            "Successfully installed yi-0.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install the custom language object from Cadet \n",
        "!python -m spacy project run install /content/newlang_project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step generates a config.cfg file that will be used for model training.  We recommend that you use Weights and Biases to monitor model training. You'll need to create an account at [wandb.ai](https://wandb.ai/site) and get an API key.  "
      ],
      "metadata": {
        "id": "lpCds9dP30R8"
      },
      "id": "lpCds9dP30R8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "83b97dbe",
      "metadata": {
        "id": "83b97dbe",
        "outputId": "8b931400-f32c-44d8-b3b6-06b2f23e8886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== config ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy init config config.cfg --lang yi -F\n",
            "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
            "- Language: yi\n",
            "- Pipeline: tagger, parser, ner\n",
            "- Optimize for: efficiency\n",
            "- Hardware: CPU\n",
            "- Transformer: None\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
            "Running command: /usr/bin/python3 scripts/update_config.py urban-giggle false\n"
          ]
        }
      ],
      "source": [
        "# Create training config\n",
        "!python -m spacy project run config /content/newlang_project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Prepare the Data for Training"
      ],
      "metadata": {
        "id": "qU_AqRK6LZrF"
      },
      "id": "qU_AqRK6LZrF"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (optional) cell to corrects a problem when your tokens have no pos value\n",
        "%%writefile /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n",
        "import re\n",
        "\n",
        "from .conll_ner_to_docs import n_sents_info\n",
        "from ...training import iob_to_biluo, biluo_tags_to_spans\n",
        "from ...tokens import Doc, Token, Span\n",
        "from ...vocab import Vocab\n",
        "from wasabi import Printer\n",
        "\n",
        "\n",
        "def conllu_to_docs(\n",
        "    input_data,\n",
        "    n_sents=10,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "    merge_subtokens=False,\n",
        "    no_print=False,\n",
        "    **_\n",
        "):\n",
        "    \"\"\"\n",
        "    Convert conllu files into JSON format for use with train cli.\n",
        "    append_morphology parameter enables appending morphology to tags, which is\n",
        "    useful for languages such as Spanish, where UD tags are not so rich.\n",
        "\n",
        "    Extract NER tags if available and convert them so that they follow\n",
        "    BILUO and the Wikipedia scheme\n",
        "    \"\"\"\n",
        "    MISC_NER_PATTERN = \"^((?:name|NE)=)?([BILU])-([A-Z_]+)|O$\"\n",
        "    msg = Printer(no_print=no_print)\n",
        "    n_sents_info(msg, n_sents)\n",
        "    sent_docs = read_conllx(\n",
        "        input_data,\n",
        "        append_morphology=append_morphology,\n",
        "        ner_tag_pattern=MISC_NER_PATTERN,\n",
        "        ner_map=ner_map,\n",
        "        merge_subtokens=merge_subtokens,\n",
        "    )\n",
        "    sent_docs_to_merge = []\n",
        "    for sent_doc in sent_docs:\n",
        "        sent_docs_to_merge.append(sent_doc)\n",
        "        if len(sent_docs_to_merge) % n_sents == 0:\n",
        "            yield Doc.from_docs(sent_docs_to_merge)\n",
        "            sent_docs_to_merge = []\n",
        "    if sent_docs_to_merge:\n",
        "        yield Doc.from_docs(sent_docs_to_merge)\n",
        "\n",
        "\n",
        "def has_ner(input_data, ner_tag_pattern):\n",
        "    \"\"\"\n",
        "    Check the MISC column for NER tags.\n",
        "    \"\"\"\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            for line in lines:\n",
        "                parts = line.split(\"\\t\")\n",
        "                id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "                for misc_part in misc.split(\"|\"):\n",
        "                    if re.match(ner_tag_pattern, misc_part):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def read_conllx(\n",
        "    input_data,\n",
        "    append_morphology=False,\n",
        "    merge_subtokens=False,\n",
        "    ner_tag_pattern=\"\",\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Yield docs, one for each sentence\"\"\"\n",
        "    vocab = Vocab()  # need vocab to make a minimal Doc\n",
        "    for sent in input_data.strip().split(\"\\n\\n\"):\n",
        "        lines = sent.strip().split(\"\\n\")\n",
        "        if lines:\n",
        "            while lines[0].startswith(\"#\"):\n",
        "                lines.pop(0)\n",
        "            doc = conllu_sentence_to_doc(\n",
        "                vocab,\n",
        "                lines,\n",
        "                ner_tag_pattern,\n",
        "                merge_subtokens=merge_subtokens,\n",
        "                append_morphology=append_morphology,\n",
        "                ner_map=ner_map,\n",
        "            )\n",
        "            yield doc\n",
        "\n",
        "\n",
        "def get_entities(lines, tag_pattern, ner_map=None):\n",
        "    \"\"\"Find entities in the MISC column according to the pattern and map to\n",
        "    final entity type with `ner_map` if mapping present. Entity tag is 'O' if\n",
        "    the pattern is not matched.\n",
        "\n",
        "    lines (str): CONLL-U lines for one sentences\n",
        "    tag_pattern (str): Regex pattern for entity tag\n",
        "    ner_map (dict): Map old NER tag names to new ones, '' maps to O.\n",
        "    RETURNS (list): List of BILUO entity tags\n",
        "    \"\"\"\n",
        "    miscs = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_ or \".\" in id_:\n",
        "            continue\n",
        "        miscs.append(misc)\n",
        "\n",
        "    iob = []\n",
        "    for misc in miscs:\n",
        "        iob_tag = \"O\"\n",
        "        for misc_part in misc.split(\"|\"):\n",
        "            tag_match = re.match(tag_pattern, misc_part)\n",
        "            if tag_match:\n",
        "                prefix = tag_match.group(2)\n",
        "                suffix = tag_match.group(3)\n",
        "                if prefix and suffix:\n",
        "                    iob_tag = prefix + \"-\" + suffix\n",
        "                    if ner_map:\n",
        "                        suffix = ner_map.get(suffix, suffix)\n",
        "                        if suffix == \"\":\n",
        "                            iob_tag = \"O\"\n",
        "                        else:\n",
        "                            iob_tag = prefix + \"-\" + suffix\n",
        "                break\n",
        "        iob.append(iob_tag)\n",
        "    return iob_to_biluo(iob)\n",
        "\n",
        "\n",
        "def conllu_sentence_to_doc(\n",
        "    vocab,\n",
        "    lines,\n",
        "    ner_tag_pattern,\n",
        "    merge_subtokens=False,\n",
        "    append_morphology=False,\n",
        "    ner_map=None,\n",
        "):\n",
        "    \"\"\"Create an Example from the lines for one CoNLL-U sentence, merging\n",
        "    subtokens and appending morphology to tags if required.\n",
        "\n",
        "    lines (str): The non-comment lines for a CoNLL-U sentence\n",
        "    ner_tag_pattern (str): The regex pattern for matching NER in MISC col\n",
        "    RETURNS (Example): An example containing the annotation\n",
        "    \"\"\"\n",
        "    # create a Doc with each subtoken as its own token\n",
        "    # if merging subtokens, each subtoken orth is the merged subtoken form\n",
        "    if not Token.has_extension(\"merged_orth\"):\n",
        "        Token.set_extension(\"merged_orth\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_lemma\"):\n",
        "        Token.set_extension(\"merged_lemma\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_morph\"):\n",
        "        Token.set_extension(\"merged_morph\", default=\"\")\n",
        "    if not Token.has_extension(\"merged_spaceafter\"):\n",
        "        Token.set_extension(\"merged_spaceafter\", default=\"\")\n",
        "    words, spaces, tags, poses, morphs, lemmas = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    subtok_word = \"\"\n",
        "    in_subtok = False\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \".\" in id_:\n",
        "            continue\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "        if \"-\" in id_:\n",
        "            in_subtok = True\n",
        "            subtok_word = word\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_spaceafter = \"SpaceAfter=No\" not in misc\n",
        "            continue\n",
        "        if merge_subtokens and in_subtok:\n",
        "            words.append(subtok_word)\n",
        "        else:\n",
        "            words.append(word)\n",
        "        if in_subtok:\n",
        "            if id_ == subtok_end:\n",
        "                spaces.append(subtok_spaceafter)\n",
        "            else:\n",
        "                spaces.append(False)\n",
        "        elif \"SpaceAfter=No\" in misc:\n",
        "            spaces.append(False)\n",
        "        else:\n",
        "            spaces.append(True)\n",
        "        if in_subtok and id_ == subtok_end:\n",
        "            subtok_word = \"\"\n",
        "            in_subtok = False\n",
        "        id_ = int(id_) - 1\n",
        "        head = (int(head) - 1) if head not in (\"0\", \"_\") else id_\n",
        "        tag = pos if tag == \"_\" else tag\n",
        "        morph = morph if morph != \"_\" else \"\"\n",
        "        dep = \"ROOT\" if dep == \"root\" else dep\n",
        "        lemmas.append(lemma)\n",
        "        if pos == \"_\":\n",
        "            pos = \"\"\n",
        "        poses.append(pos)\n",
        "        tags.append(tag)\n",
        "        morphs.append(morph)\n",
        "        heads.append(head)\n",
        "        deps.append(dep)\n",
        "\n",
        "    doc = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        lemmas=lemmas,\n",
        "        morphs=morphs,\n",
        "        heads=heads,\n",
        "    )\n",
        "    for i in range(len(doc)):\n",
        "        doc[i]._.merged_orth = words[i]\n",
        "        doc[i]._.merged_morph = morphs[i]\n",
        "        doc[i]._.merged_lemma = lemmas[i]\n",
        "        doc[i]._.merged_spaceafter = spaces[i]\n",
        "    ents = get_entities(lines, ner_tag_pattern, ner_map)\n",
        "    doc.ents = biluo_tags_to_spans(doc, ents)\n",
        "\n",
        "    if merge_subtokens:\n",
        "        doc = merge_conllu_subtokens(lines, doc)\n",
        "\n",
        "    # create final Doc from custom Doc annotation\n",
        "    words, spaces, tags, morphs, lemmas, poses = [], [], [], [], [], []\n",
        "    heads, deps = [], []\n",
        "    for i, t in enumerate(doc):\n",
        "        words.append(t._.merged_orth)\n",
        "        lemmas.append(t._.merged_lemma)\n",
        "        spaces.append(t._.merged_spaceafter)\n",
        "        morphs.append(t._.merged_morph)\n",
        "        if append_morphology and t._.merged_morph:\n",
        "            tags.append(t.tag_ + \"__\" + t._.merged_morph)\n",
        "        else:\n",
        "            tags.append(t.tag_)\n",
        "        poses.append(t.pos_)\n",
        "        heads.append(t.head.i)\n",
        "        deps.append(t.dep_)\n",
        "\n",
        "    doc_x = Doc(\n",
        "        vocab,\n",
        "        words=words,\n",
        "        spaces=spaces,\n",
        "        tags=tags,\n",
        "        morphs=morphs,\n",
        "        lemmas=lemmas,\n",
        "        pos=poses,\n",
        "        deps=deps,\n",
        "        heads=heads,\n",
        "    )\n",
        "    doc_x.ents = [Span(doc_x, ent.start, ent.end, label=ent.label) for ent in doc.ents]\n",
        "\n",
        "    return doc_x\n",
        "\n",
        "\n",
        "def merge_conllu_subtokens(lines, doc):\n",
        "    # identify and process all subtoken spans to prepare attrs for merging\n",
        "    subtok_spans = []\n",
        "    for line in lines:\n",
        "        parts = line.split(\"\\t\")\n",
        "        id_, word, lemma, pos, tag, morph, head, dep, _1, misc = parts\n",
        "        if \"-\" in id_:\n",
        "            subtok_start, subtok_end = id_.split(\"-\")\n",
        "            subtok_span = doc[int(subtok_start) - 1 : int(subtok_end)]\n",
        "            subtok_spans.append(subtok_span)\n",
        "            # create merged tag, morph, and lemma values\n",
        "            tags = []\n",
        "            morphs = {}\n",
        "            lemmas = []\n",
        "            for token in subtok_span:\n",
        "                tags.append(token.tag_)\n",
        "                lemmas.append(token.lemma_)\n",
        "                if token._.merged_morph:\n",
        "                    for feature in token._.merged_morph.split(\"|\"):\n",
        "                        field, values = feature.split(\"=\", 1)\n",
        "                        if field not in morphs:\n",
        "                            morphs[field] = set()\n",
        "                        for value in values.split(\",\"):\n",
        "                            morphs[field].add(value)\n",
        "            # create merged features for each morph field\n",
        "            for field, values in morphs.items():\n",
        "                morphs[field] = field + \"=\" + \",\".join(sorted(values))\n",
        "            # set the same attrs on all subtok tokens so that whatever head the\n",
        "            # retokenizer chooses, the final attrs are available on that token\n",
        "            for token in subtok_span:\n",
        "                token._.merged_orth = token.orth_\n",
        "                token._.merged_lemma = \" \".join(lemmas)\n",
        "                token.tag_ = \"_\".join(tags)\n",
        "                token._.merged_morph = \"|\".join(sorted(morphs.values()))\n",
        "                token._.merged_spaceafter = (\n",
        "                    True if subtok_span[-1].whitespace_ else False\n",
        "                )\n",
        "\n",
        "    with doc.retokenize() as retokenizer:\n",
        "        for span in subtok_spans:\n",
        "            retokenizer.merge(span)\n",
        "\n",
        "    return doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "HPjD0SN6iAvc",
        "outputId": "f38fc69a-ba2f-4f35-a2c3-3cc440a2b68f"
      },
      "id": "HPjD0SN6iAvc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /usr/local/lib/python3.7/dist-packages/spacy/training/converters/conllu_to_docs.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "563fdc94",
      "metadata": {
        "id": "563fdc94",
        "outputId": "eefb7496-da69-4e02-d5dc-978b8783bf3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== convert ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/convert.py assets/urban-giggle/3_inception_export 10 yi\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (50 documents):\n",
            "corpus/converted/he_htb-ud-test.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (525 documents):\n",
            "corpus/converted/he_htb-ud-train.spacy\u001b[0m\n",
            "\u001b[38;5;4mℹ Grouping every 10 sentences into a document.\u001b[0m\n",
            "\u001b[38;5;2m✔ Generated output file (49 documents):\n",
            "corpus/converted/he_htb-ud-dev.spacy\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Convert the conllu files from inception to spaCy binary format\n",
        "# Read the conll files with ner data and as ents to spaCy docs \n",
        "!python -m spacy project run convert /content/newlang_project -F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9519c858",
      "metadata": {
        "id": "9519c858",
        "outputId": "37b356d3-bce9-4440-f3eb-48e088c4153f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== split ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 scripts/split.py 0.2 11 yi\n",
            "🚂 Created 499 training docs\n",
            "😊 Created 100 validation docs\n",
            "🧪  Created 25 test docs\n"
          ]
        }
      ],
      "source": [
        "# test/train split \n",
        "!python -m spacy project run split /content/newlang_project "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4feefe6f",
      "metadata": {
        "id": "4feefe6f",
        "outputId": "8df2bdf8-e7a8-4023-9da0-61fd2221a495",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== debug ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy debug data ./config.cfg\n",
            "\u001b[1m\n",
            "============================ Data file validation ============================\u001b[0m\n",
            "\u001b[38;5;2m✔ Pipeline can be initialized with data\u001b[0m\n",
            "\u001b[38;5;2m✔ Corpus is loadable\u001b[0m\n",
            "\u001b[1m\n",
            "=============================== Training stats ===============================\u001b[0m\n",
            "Language: yi\n",
            "Training pipeline: tok2vec, tagger, parser, ner\n",
            "499 training docs\n",
            "100 evaluation docs\n",
            "\u001b[38;5;2m✔ No overlap between training and evaluation data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples to train a new pipeline (499)\u001b[0m\n",
            "\u001b[1m\n",
            "============================== Vocab & Vectors ==============================\u001b[0m\n",
            "\u001b[38;5;4mℹ 128991 total word(s) in the data (15743 unique)\u001b[0m\n",
            "\u001b[38;5;3m⚠ 29747 misaligned tokens in the training data\u001b[0m\n",
            "\u001b[38;5;3m⚠ 5935 misaligned tokens in the dev data\u001b[0m\n",
            "\u001b[38;5;4mℹ No word vectors present in the package\u001b[0m\n",
            "\u001b[1m\n",
            "========================== Named Entity Recognition ==========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 0 label(s)\u001b[0m\n",
            "0 missing value(s) (tokens with '-' label)\n",
            "\u001b[38;5;2m✔ Good amount of examples for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ Examples without occurrences available for all labels\u001b[0m\n",
            "\u001b[38;5;2m✔ No entities consisting of or starting/ending with whitespace\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Part-of-speech Tagging ===========================\u001b[0m\n",
            "\u001b[38;5;4mℹ 15 label(s) in train data\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Dependency Parsing =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 4716 sentence(s) with an average length of 27.4 words.\u001b[0m\n",
            "\u001b[38;5;4mℹ Found 114 nonprojective train sentence(s)\u001b[0m\n",
            "\u001b[38;5;4mℹ 36 label(s) in train data\u001b[0m\n",
            "\u001b[38;5;4mℹ 54 label(s) in projectivized train data\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'dislocated' (8)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'discourse' (2)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for label 'csubj' (1)\u001b[0m\n",
            "\u001b[38;5;3m⚠ Low number of examples for 15 label(s) in the projectivized\n",
            "dependency trees used for training. You may want to projectivize labels such as\n",
            "punct before training in order to improve parser performance.\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Summary ==================================\u001b[0m\n",
            "\u001b[38;5;2m✔ 6 checks passed\u001b[0m\n",
            "\u001b[38;5;3m⚠ 10 warnings\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Debug the data\n",
        "!python -m spacy project run debug /content/newlang_project "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Model Training "
      ],
      "metadata": {
        "id": "151-cj1dLgAD"
      },
      "id": "151-cj1dLgAD"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "38b490a4",
      "metadata": {
        "id": "38b490a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7c0673-3dd5-4ddc-ce1e-e093c346db5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "=================================== train ===================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy train config.cfg --output training/urban-giggle --gpu-id 0 --nlp.lang=yi\n",
            "\u001b[38;5;2m✔ Created output directory: training/urban-giggle\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: training/urban-giggle\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2022-01-02 15:47:57,331] [INFO] Set up nlp object from config\n",
            "[2022-01-02 15:47:57,347] [INFO] Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "[2022-01-02 15:47:57,354] [INFO] Created vocabulary\n",
            "[2022-01-02 15:47:57,356] [INFO] Finished initializing nlp object\n",
            "[2022-01-02 15:49:26,486] [INFO] Initialized pipeline components: ['tok2vec', 'tagger', 'parser', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger', 'parser', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
            "E    #       LOSS TOK2VEC  LOSS TAGGER  LOSS PARSER  LOSS NER  TAG_ACC  DEP_UAS  DEP_LAS  SENTS_F  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  ------------  -----------  -----------  --------  -------  -------  -------  -------  ------  ------  ------  ------\n",
            "  0       0          0.00       100.80       312.22     81.00    20.91     4.10     4.09     0.01    0.00    0.00    0.00    0.08\n",
            "  0     200       2396.25     11396.54     27202.24    289.42    51.68    21.70    14.61    74.80    0.00    0.00    0.00    0.23\n",
            "  0     400       4349.41      6347.41     21101.19      0.00    56.53    24.52    18.34    77.80    0.00    0.00    0.00    0.26\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "training/urban-giggle/model-last\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "!python -m spacy project run train /content/newlang_project "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you get `ValueError: Could not find gold transition - see logs above.`  \n",
        "You may not have sufficent data to train on: https://github.com/explosion/spaCy/discussions/7282"
      ],
      "metadata": {
        "id": "ynXr8vlXqCxv"
      },
      "id": "ynXr8vlXqCxv"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "018362d8",
      "metadata": {
        "id": "018362d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1198836a-6d4a-4f3e-8907-b7757f71e24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== evaluate ==================================\u001b[0m\n",
            "Running command: /usr/bin/python3 -m spacy evaluate ./training/urban-giggle/model-best ./corpus/converted/test.spacy --output ./metrics/urban-giggle.json --gpu-id 0\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK      80.24\n",
            "TAG      56.03\n",
            "UAS      22.95\n",
            "LAS      16.97\n",
            "NER P    -    \n",
            "NER R    -    \n",
            "NER F    -    \n",
            "SENT P   73.96\n",
            "SENT R   85.20\n",
            "SENT F   79.18\n",
            "SPEED    12587\n",
            "\n",
            "\u001b[1m\n",
            "=============================== LAS (per type) ===============================\u001b[0m\n",
            "\n",
            "                       P       R       F\n",
            "cc                 60.00    6.03   10.96\n",
            "root               51.05   48.40   49.69\n",
            "advmod             56.63   39.83   46.77\n",
            "det                80.56    3.87    7.38\n",
            "nsubj              48.15   24.59   32.56\n",
            "obl                38.18    4.73    8.42\n",
            "conj               10.29    6.28    7.80\n",
            "compound:smixut    41.56    9.25   15.13\n",
            "case               84.96   12.58   21.92\n",
            "case:gen           94.23   24.38   38.74\n",
            "nmod:poss           7.69    0.60    1.10\n",
            "mark               64.29    7.53   13.48\n",
            "fixed              12.90    7.02    9.09\n",
            "advcl               4.92    4.23    4.55\n",
            "case:acc           92.86   29.55   44.83\n",
            "obj                36.73   10.40   16.22\n",
            "nsubj:cop         100.00    5.26   10.00\n",
            "cop                77.78   48.28   59.57\n",
            "acl:relcl          22.22    1.56    2.92\n",
            "nummod             80.49   51.56   62.86\n",
            "compound:affix     50.00   16.67   25.00\n",
            "nmod               38.10    2.85    5.30\n",
            "acl                25.00    4.00    6.90\n",
            "amod               27.63   25.40   26.47\n",
            "ccomp              33.33   21.21   25.93\n",
            "xcomp              37.50   24.32   29.51\n",
            "mark:q              0.00    0.00    0.00\n",
            "dep                 0.00    0.00    0.00\n",
            "appos               5.88    2.17    3.17\n",
            "aux                57.50   65.71   61.33\n",
            "flat:name          41.94   18.06   25.24\n",
            "parataxis           0.00    0.00    0.00\n",
            "\n",
            "\u001b[38;5;2m✔ Saved results to metrics/urban-giggle.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model using the test data\n",
        "!python -m spacy project run evaluate /content/newlang_project "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the path for your meta.json file\n",
        "# You'll need to add newlang_project/ +  the path from the training step just after \"✔ Saved pipeline to output directory\"\n",
        "!ls newlang_project/training/urban-giggle/model-last"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg1sLlVrgiyu",
        "outputId": "09aa97b7-f7b3-404b-f965-b9140b60e3e6"
      },
      "id": "gg1sLlVrgiyu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.cfg  meta.json  ner  parser  tagger  tok2vec  tokenizer\tvocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Update meta.json\n",
        "import spacy \n",
        "import srsly \n",
        "\n",
        "# Change path to match that from the training cell where it says \"✔ Saved pipeline to output directory\"\n",
        "meta_path = \"newlang_project/training/urban-giggle/model-last/meta.json\"\n",
        "\n",
        "# Replace values below for your project\n",
        "my_meta = { \n",
        "    \"lang\":\"yi\",\n",
        "    \"name\":\"yiddish_sm\",\n",
        "    \"version\":\"0.0.1\",\n",
        "    \"description\":\"Yiddish pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, lemmatizer.\",\n",
        "    \"author\":\"New Languages for NLP\",\n",
        "    \"email\":\"newnlp@princeton.edu\",\n",
        "    \"url\":\"https://newnlp.princeton.edu\",\n",
        "    \"license\":\"MIT\", \n",
        "    }\n",
        "meta = spacy.util.load_meta(meta_path)\n",
        "meta.update(my_meta)\n",
        "srsly.write_json(meta_path, meta)\n",
        "meta"
      ],
      "metadata": {
        "id": "3zGCTURr9JE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861443f2-a93b-4022-dd1b-10faa79599be"
      },
      "id": "3zGCTURr9JE6",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'author': 'New Languages for NLP',\n",
              " 'components': ['tok2vec', 'tagger', 'parser', 'ner'],\n",
              " 'description': 'Yiddish pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, lemmatizer.',\n",
              " 'disabled': [],\n",
              " 'email': 'newnlp@princeton.edu',\n",
              " 'labels': {'ner': [],\n",
              "  'parser': ['ROOT',\n",
              "   'acl',\n",
              "   'acl:relcl',\n",
              "   'advcl',\n",
              "   'advmod',\n",
              "   'amod',\n",
              "   'appos',\n",
              "   'aux',\n",
              "   'case',\n",
              "   'case:acc',\n",
              "   'case:gen',\n",
              "   'cc',\n",
              "   'ccomp',\n",
              "   'compound:affix',\n",
              "   'compound:smixut',\n",
              "   'conj',\n",
              "   'cop',\n",
              "   'dep',\n",
              "   'det',\n",
              "   'fixed',\n",
              "   'flat:name',\n",
              "   'mark',\n",
              "   'mark:q',\n",
              "   'nmod',\n",
              "   'nmod:poss',\n",
              "   'nsubj',\n",
              "   'nsubj:cop',\n",
              "   'nummod',\n",
              "   'obj',\n",
              "   'obl',\n",
              "   'parataxis',\n",
              "   'punct',\n",
              "   'xcomp'],\n",
              "  'tagger': ['ADJ',\n",
              "   'ADP',\n",
              "   'ADV',\n",
              "   'AUX',\n",
              "   'CCONJ',\n",
              "   'DET',\n",
              "   'INTJ',\n",
              "   'NOUN',\n",
              "   'NUM',\n",
              "   'PRON',\n",
              "   'PROPN',\n",
              "   'PUNCT',\n",
              "   'SCONJ',\n",
              "   'VERB',\n",
              "   'X'],\n",
              "  'tok2vec': []},\n",
              " 'lang': 'yi',\n",
              " 'license': 'MIT',\n",
              " 'name': 'yiddish_sm',\n",
              " 'performance': {'dep_las': 0.1834272337,\n",
              "  'dep_las_per_type': {'acl': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'acl:relcl': {'f': 0.016393442600000002, 'p': 0.16, 'r': 0.0086393089},\n",
              "   'advcl': {'f': 0.115, 'p': 0.11442786070000001, 'r': 0.11557788940000001},\n",
              "   'advmod': {'f': 0.4551451187, 'p': 0.5458860759, 'r': 0.39027149320000004},\n",
              "   'amod': {'f': 0.2783505155, 'p': 0.2900390625, 'r': 0.2675675676},\n",
              "   'appos': {'f': 0.1276595745, 'p': 0.23684210530000002, 'r': 0.0873786408},\n",
              "   'aux': {'f': 0.5, 'p': 0.49137931030000004, 'r': 0.5089285714},\n",
              "   'case': {'f': 0.2063086104, 'p': 0.7960526316000001, 'r': 0.1185112635},\n",
              "   'case:acc': {'f': 0.42708333330000003,\n",
              "    'p': 0.9879518072000001,\n",
              "    'r': 0.2724252492},\n",
              "   'case:gen': {'f': 0.4763741562, 'p': 0.9320754717, 'r': 0.3199481865},\n",
              "   'cc': {'f': 0.1263669502, 'p': 0.6582278481, 'r': 0.0698924731},\n",
              "   'ccomp': {'f': 0.1785714286, 'p': 0.2739726027, 'r': 0.1324503311},\n",
              "   'compound:affix': {'f': 0.2413793103, 'p': 0.5384615385, 'r': 0.1555555556},\n",
              "   'compound:smixut': {'f': 0.1926006529,\n",
              "    'p': 0.4154929577,\n",
              "    'r': 0.12535410760000001},\n",
              "   'conj': {'f': 0.0822510823,\n",
              "    'p': 0.11176470590000001,\n",
              "    'r': 0.06506849320000001},\n",
              "   'cop': {'f': 0.4742268041, 'p': 0.5609756098, 'r': 0.4107142857},\n",
              "   'csubj': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'dep': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'det': {'f': 0.062049514300000004, 'p': 0.7279411765, 'r': 0.032405892},\n",
              "   'dislocated': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'fixed': {'f': 0.0997375328, 'p': 0.1557377049, 'r': 0.0733590734},\n",
              "   'flat:name': {'f': 0.2706766917,\n",
              "    'p': 0.5844155844000001,\n",
              "    'r': 0.1761252446},\n",
              "   'mark': {'f': 0.1985370951, 'p': 0.6985294118, 'r': 0.1157125457},\n",
              "   'mark:q': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
              "   'nmod': {'f': 0.026711185300000002,\n",
              "    'p': 0.24242424240000002,\n",
              "    'r': 0.014134275600000001},\n",
              "   'nmod:poss': {'f': 0.0863697706, 'p': 0.3902439024, 'r': 0.0485584219},\n",
              "   'nsubj': {'f': 0.36658932710000003, 'p': 0.5197368421, 'r': 0.2831541219},\n",
              "   'nsubj:cop': {'f': 0.04, 'p': 0.25, 'r': 0.0217391304},\n",
              "   'nummod': {'f': 0.5932452276, 'p': 0.7062937063, 'r': 0.5113924051000001},\n",
              "   'obj': {'f': 0.25030978930000003, 'p': 0.4832535885, 'r': 0.1688963211},\n",
              "   'obl': {'f': 0.1009615385, 'p': 0.4525862069, 'r': 0.0568181818},\n",
              "   'parataxis': {'f': 0.1034482759,\n",
              "    'p': 0.4285714286,\n",
              "    'r': 0.058823529400000005},\n",
              "   'root': {'f': 0.5544252289, 'p': 0.5641821946000001, 'r': 0.545},\n",
              "   'xcomp': {'f': 0.3188405797, 'p': 0.4489795918, 'r': 0.2471910112}},\n",
              "  'dep_uas': 0.245169921,\n",
              "  'ents_f': 0.0,\n",
              "  'ents_p': 0.0,\n",
              "  'ents_per_type': 0.0,\n",
              "  'ents_r': 0.0,\n",
              "  'ner_loss': 4.61146e-05,\n",
              "  'parser_loss': 21101.1858311111,\n",
              "  'sents_f': 0.7780341486,\n",
              "  'sents_p': 0.7223650386,\n",
              "  'sents_r': 0.843,\n",
              "  'tag_acc': 0.5652802147,\n",
              "  'tagger_loss': 6347.410946846,\n",
              "  'tok2vec_loss': 4349.4050883393},\n",
              " 'pipeline': ['tok2vec', 'tagger', 'parser', 'ner'],\n",
              " 'spacy_git_version': 'bb26550e2',\n",
              " 'spacy_version': '>=3.2.0,<3.3.0',\n",
              " 'url': 'https://newnlp.princeton.edu',\n",
              " 'vectors': {'keys': 0,\n",
              "  'mode': 'default',\n",
              "  'name': None,\n",
              "  'vectors': 0,\n",
              "  'width': 0},\n",
              " 'version': '0.0.1'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the trained model to your computer.\n"
      ],
      "metadata": {
        "id": "JM309FhNVAeb"
      },
      "id": "JM309FhNVAeb"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8e1d6f36",
      "metadata": {
        "id": "8e1d6f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd6b19c-0f93-47a9-eae3-ee1b67abfa3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Building package artifacts: sdist\u001b[0m\n",
            "\u001b[38;5;2m✔ Loaded meta.json from file\u001b[0m\n",
            "newlang_project/training/urban-giggle/model-last/meta.json\n",
            "\u001b[38;5;2m✔ Generated README.md from meta.json\u001b[0m\n",
            "\u001b[38;5;2m✔ Successfully created package 'yi_yiddish_sm-0.0.1'\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1\n",
            "running sdist\n",
            "running egg_info\n",
            "creating yi_yiddish_sm.egg-info\n",
            "writing yi_yiddish_sm.egg-info/PKG-INFO\n",
            "writing dependency_links to yi_yiddish_sm.egg-info/dependency_links.txt\n",
            "writing entry points to yi_yiddish_sm.egg-info/entry_points.txt\n",
            "writing requirements to yi_yiddish_sm.egg-info/requires.txt\n",
            "writing top-level names to yi_yiddish_sm.egg-info/top_level.txt\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching 'LICENSES_SOURCES'\n",
            "writing manifest file 'yi_yiddish_sm.egg-info/SOURCES.txt'\n",
            "running check\n",
            "creating yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "creating yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying files to yi_yiddish_sm-0.0.1...\n",
            "copying MANIFEST.in -> yi_yiddish_sm-0.0.1\n",
            "copying README.md -> yi_yiddish_sm-0.0.1\n",
            "copying meta.json -> yi_yiddish_sm-0.0.1\n",
            "copying setup.py -> yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/__init__.py -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm\n",
            "copying yi_yiddish_sm.egg-info/PKG-INFO -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/SOURCES.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/dependency_links.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/entry_points.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/not-zip-safe -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/requires.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm.egg-info/top_level.txt -> yi_yiddish_sm-0.0.1/yi_yiddish_sm.egg-info\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/README.md -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/config.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/meta.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tokenizer -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/ner\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser/moves -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/parser\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tagger\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec/model -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/tok2vec\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/key2row -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/lookups.bin -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/strings.json -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "copying yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab/vectors.cfg -> yi_yiddish_sm-0.0.1/yi_yiddish_sm/yi_yiddish_sm-0.0.1/vocab\n",
            "Writing yi_yiddish_sm-0.0.1/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'yi_yiddish_sm-0.0.1' (and everything under it)\n",
            "\u001b[38;5;2m✔ Successfully created zipped Python package\u001b[0m\n",
            "newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Save the model to disk in a format that can be easily  downloaded and re-used.\n",
        "!python -m spacy package ./newlang_project/training/urban-giggle/model-last newlang_project/export "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8d32abf2",
      "metadata": {
        "id": "8d32abf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "24a11e7a-800a-4fe8-ed12-c26f8fb15146"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_85656e69-c8e3-4887-8d34-43751ee0b2d8\", \"yi_yiddish_sm-0.0.1.tar.gz\", 7877040)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# replace with the path in the previous cell under \"✔ Successfully created zipped Python package\"\n",
        "files.download('newlang_project/export/yi_yiddish_sm-0.0.1/dist/yi_yiddish_sm-0.0.1.tar.gz')\n",
        "\n",
        "# once on your computer, you can pip install en_pipeline-0.0.0.tar.gz\n",
        "# Add to 4_trained_models folder in GitHub"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "New Language Training (Colab).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}