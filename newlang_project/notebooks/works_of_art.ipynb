{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking works cited in the _Dai Nihon Shi_\n",
    "We have a selection of chapters from the _Dai Nihon Shi_ that have been manually annotated with named entities, including people (`PER`), locations (`LOC`), and works of art (`WORK_OF_ART`). Using this selection, we want to identify some of the most frequently mentioned works of art (nearly always written works) and track their appearance across the entire _Dai Nihon Shi_.\n",
    "\n",
    "First, let's find all the works of art (tagged `WORK_OF_ART`) in our annotated data, which is stored in the CoNLL-2002 (`.conll`) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日本紀略', '三代實錄', '日本起略', '一代要記', '三代實錄']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "\n",
    "# load all annotated data exported from INCEpTION; make a Doc out of each chapter\n",
    "docs = []\n",
    "nlp = spacy.blank(\"lzh\")\n",
    "for conll_file in Path(\"../assets/kanbun/3_inception_export/\").glob(\"*.conll\"):\n",
    "  with open(conll_file, \"r\") as f:\n",
    "    conll_doc = f.read().strip()\n",
    "    words = []\n",
    "    sent_starts = []\n",
    "    pos_tags = []\n",
    "    biluo_tags = []\n",
    "    for conll_sent in conll_doc.split(\"\\n\\n\"):\n",
    "        conll_sent = conll_sent.strip()\n",
    "        if not conll_sent:\n",
    "            continue\n",
    "        lines = [line.strip() for line in conll_sent.split(\"\\n\") if line.strip()]\n",
    "        cols = list(zip(*[line.split() for line in lines]))\n",
    "        length = len(cols[0])\n",
    "        words.extend(cols[0])\n",
    "        sent_starts.extend([True] + [False] * (length - 1))\n",
    "        biluo_tags.extend(spacy.training.iob_utils.iob_to_biluo(cols[-1]))\n",
    "        pos_tags.extend(cols[1] if len(cols) > 2 else [\"-\"] * length)\n",
    "    doc = spacy.tokens.Doc(\n",
    "      nlp.vocab,\n",
    "      words=words,\n",
    "      spaces=[False] * len(words),\n",
    "    )\n",
    "    for i, token in enumerate(doc):\n",
    "      token.tag_ = pos_tags[i]\n",
    "      token.is_sent_start = sent_starts[i]\n",
    "    entities = spacy.training.iob_utils.tags_to_entities(biluo_tags)\n",
    "    doc.ents = [spacy.tokens.Span(doc, start=s, end=e + 1, label=L) for L, s, e in entities]\n",
    "    docs.append(doc)\n",
    "\n",
    "# get the entities (people, places, works) from each chapter\n",
    "entities = {\n",
    "  \"people\": [],\n",
    "  \"places\": [],\n",
    "  \"works\": [],\n",
    "}\n",
    "for doc in docs:\n",
    "  for ent in doc.ents:\n",
    "    if ent.label_ == \"PERSON\":\n",
    "      entities[\"people\"].append(ent)\n",
    "    elif ent.label_ == \"GPE\":\n",
    "      entities[\"places\"].append(ent)\n",
    "    elif ent.label_ == \"WORK_OF_ART\":\n",
    "      entities[\"works\"].append(ent)\n",
    "\n",
    "# check some of the annotated works\n",
    "print([ent.text for ent in entities[\"works\"][:5]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "399f58a1e2dd97b84b46c9a3caa54f8afa6f90330ebb1f6b22c94095f857003f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('kanbun': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
