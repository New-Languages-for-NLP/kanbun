{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking works cited in the _Dai Nihon Shi_\n",
    "We have a selection of chapters from the _Dai Nihon Shi_ that have been manually annotated with named entities, including people (`PER`), locations (`LOC`), and works of art (`WORK_OF_ART`). Using this selection, we want to identify some of the most frequently mentioned works of art (nearly always written works) and track their appearance across the entire _Dai Nihon Shi_.\n",
    "\n",
    "First, let's check the works of art (tagged `WORK_OF_ART`) in our annotated data, which is stored in the CoNLL-2002 (`.conll`) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickbudak/.pyenv/versions/kanbun/lib/python3.9/site-packages/spacy/displacy/__init__.py:200: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">藤原兼實，關白忠通第三子也。家號九條。保元三年，敘正五位下，累進至左近衛中將。平治中，為從三位權中納言。應保元年，轉權大納言，兼右近衛大將。長寬二年，為內大臣。仁安初，兼東宮傅。亡幾，轉右大臣。承安末</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spacy\n",
    "\n",
    "# load all annotated data exported from INCEpTION; make a Doc out of each chapter\n",
    "docs = []\n",
    "nlp = spacy.blank(\"lzh\")\n",
    "for conll_file in Path(\"../assets/kanbun/3_inception_export/\").glob(\"*.conll\"):\n",
    "  with open(conll_file, \"r\") as f:\n",
    "    conll_doc = f.read().strip()\n",
    "    words = []\n",
    "    sent_starts = []\n",
    "    pos_tags = []\n",
    "    biluo_tags = []\n",
    "    for conll_sent in conll_doc.split(\"\\n\\n\"):\n",
    "        conll_sent = conll_sent.strip()\n",
    "        if not conll_sent:\n",
    "            continue\n",
    "        lines = [line.strip() for line in conll_sent.split(\"\\n\") if line.strip()]\n",
    "        cols = list(zip(*[line.split() for line in lines]))\n",
    "        length = len(cols[0])\n",
    "        words.extend(cols[0])\n",
    "        sent_starts.extend([True] + [False] * (length - 1))\n",
    "        biluo_tags.extend(spacy.training.iob_utils.iob_to_biluo(cols[-1]))\n",
    "        pos_tags.extend(cols[1] if len(cols) > 2 else [\"-\"] * length)\n",
    "    doc = spacy.tokens.Doc(\n",
    "      nlp.vocab,\n",
    "      words=words,\n",
    "      spaces=[False] * len(words),\n",
    "      user_data={\"title\": conll_file.stem},\n",
    "    )\n",
    "    for i, token in enumerate(doc):\n",
    "      token.tag_ = pos_tags[i]\n",
    "      token.is_sent_start = sent_starts[i]\n",
    "    entities = spacy.training.iob_utils.tags_to_entities(biluo_tags)\n",
    "    doc.ents = [spacy.tokens.Span(doc, start=s, end=e + 1, label=L) for L, s, e in entities]\n",
    "    docs.append(doc)\n",
    "\n",
    "# check some of the annotated entities in a doc\n",
    "spacy.displacy.render(docs[0][:100], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify the 25 most frequently cited works of art from our entire annotated selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "公卿補任: 1144\n",
      "尊卑分脈: 1030\n",
      "東鑑: 803\n",
      "源平盛衰記: 737\n",
      "一代要記: 723\n",
      "日本紀略: 614\n",
      "太平記: 573\n",
      "皇胤紹運錄: 543\n",
      "日本紀: 542\n",
      "平家物語: 436\n",
      "女院小傳: 379\n",
      "續日本紀: 371\n",
      "三代實錄: 356\n",
      "歷代皇紀: 283\n",
      "扶桑略記: 268\n",
      "本書: 265\n",
      "榮華物語: 262\n",
      "增鏡: 240\n",
      "今鏡: 223\n",
      "玉海: 217\n",
      "大鏡: 202\n",
      "帝王編年記: 193\n",
      "愚管抄: 180\n",
      "百鍊抄: 173\n",
      "文德實錄: 173\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# get all the tagged works of art; count how many times each occurs\n",
    "works_of_art = [ent for doc in docs for ent in doc.ents if ent.label_ == \"WORK_OF_ART\"]\n",
    "top_works = dict(collections.Counter(ent.text for ent in works_of_art).most_common(25))\n",
    "\n",
    "print(\"\\n\".join([f\"{work}: {str(count)}\" for work, count in top_works.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get track the citation of these works of art for each of the chapters in our selection and write the output to JSON so that it can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# get the top 10 most cited works per chapter\n",
    "works_by_doc = {\n",
    "  doc.user_data[\"title\"]: dict(collections.Counter([ent.text for ent in doc.ents if ent.text in top_works]).most_common())\n",
    "  for doc in docs\n",
    "}\n",
    "\n",
    "# get all unique chapter and work titles\n",
    "doc_titles = set([doc.user_data[\"title\"] for doc in docs])\n",
    "work_titles = set([work for work_list in works_by_doc.values() for work in work_list])\n",
    "\n",
    "# create a list of matrix cells where each cell is the number of citations of a work in a chapter\n",
    "output = [\n",
    "  {\n",
    "    \"doc_title\": doc.replace(\"text\", \"\").replace(\"part\", \".\"),\n",
    "    \"work_title\": work,\n",
    "    \"count\": works_by_doc[doc][work]\n",
    "  }\n",
    " for doc, work in itertools.product(doc_titles, work_titles) if work in works_by_doc[doc]\n",
    "]\n",
    "\n",
    "# sort by chapter order\n",
    "output.sort(key=lambda row: float(row[\"doc_title\"]))\n",
    "\n",
    "# write output to json for visualization\n",
    "with open(\"output.json\", mode=\"w\") as f:\n",
    "  json.dump(output, f, ensure_ascii=False)\n",
    "\n",
    "# write output to csv\n",
    "with open(\"output.csv\", mode=\"w\") as f:\n",
    "  writer = csv.DictWriter(f, fieldnames=(\"doc\", *work_titles))\n",
    "  writer.writeheader()\n",
    "  for doc, work_list in works_by_doc.items():\n",
    "    writer.writerow({\"doc\": doc.replace(\"text\", \"\").replace(\"part\", \".\"), **work_list})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "399f58a1e2dd97b84b46c9a3caa54f8afa6f90330ebb1f6b22c94095f857003f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('kanbun': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
